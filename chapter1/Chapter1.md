# CUDA C编程及GPU基本知识

**Table Of Contents**
- [CPU](#cpu)
	* [流水线前传机制](#流水线前传机制)
  * [CPU的三级缓存](#cpu的三级缓存)
  * [什么样的问题适合GPU](#什么样的问题适合gpu)
- [GPU](#gpu)
	* [线程束wrap](#线程束wrap)
  * [线程ID](#线程id)


## CPU

### 流水线前传机制

CPU运行过程分成几步，比如分成三步，读指令，分析，执行。如果有n条指令，每条耗时t，总耗时3nt

现在让读指令之后马上读下一条指令，则分析执行的时间同时也做下一条指令的读指令分析，总耗时nt，快了三倍

### CPU的三级缓存

L1，L2，L3缓存是在CPU内部的RAM。和内存(DRAM)不同，cpu缓存是SRAM,容量小但是运行速度快。L1缓存离CPU最近，容量最小，一般1MB但运行速度是DRAM的150倍，L2远，容量大一点，运行速度慢一点，L3以此类推。缓存中存放指令和数据，程序运行时cpu先在L1缓存里找，找不到再去L2,以此类推，最后去内存里找，最慢。本质上缓存存在的意义就是cpu处理速度很快，远快于内存到cpu的数据传输，不用缓存的话cpu就一直等着没事做。
为什么分三级，不能把L1做的很大吗？处于成本和CPU体积考虑

### 什么样的问题适合GPU

计算密集， 数据并行

## GPU

### 线程束wrap

Q ： GPU的控制单元与计算单元是如何结合的， warp线程束是如何在软件和硬件端被执行的

A ： 硬件端上，一个线程束包含32线程，是硬件层面的线程集合，这些线程以不同数据执行相同指令，一个线程块被分配成多个线程束，所以线程块一般包含32的整数倍个线程。执行时，一个block里一个指令，不同thread有不同数据资源(Single-instruction Multiple-Thread). 软件端上，block是逻辑上的线程集合，Thread从block的共享内存中读指令，从Grid中的全局内存中读数据

Q ： 为什么说线程束是执行核函数的最基本单元

A ： CUDA的执行其实是线程束的执行，一个核对应一个Grid.一个Grid分出一些Block，一个Block再分成多个Thread，也就是说一个核同时只能执行一个线程束。CUDA实际上是处理32个线程（一个线程束），硬件上线程都是线性的，弄出三维索引是为了方便编程

### 线程ID

Q ： 计算Thread(3,0,0)的ID

A ： 在这个例子里Grid纬度是(2,2), Block纬度是(4,2,2)

$$
ThreadID = 3 \times 2 \times 8 + 3 = 51
$$
